{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/01/2020 22:58:30 - INFO - transformers.file_utils -   PyTorch version 1.5.0+cu92 available.\n",
      "/home/rohit/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rohit/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rohit/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rohit/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rohit/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rohit/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from farm.data_handler.data_silo import DataSilo\n",
    "from farm.data_handler.processor import TextClassificationProcessor\n",
    "from farm.modeling.optimization import initialize_optimizer\n",
    "from farm.infer import Inferencer\n",
    "from farm.modeling.adaptive_model import AdaptiveModel\n",
    "from farm.modeling.language_model import LanguageModel\n",
    "from farm.modeling.prediction_head import MultiLabelTextClassificationHead\n",
    "from farm.modeling.tokenization import Tokenizer\n",
    "from farm.train import Trainer\n",
    "from farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/01/2020 22:58:33 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n"
     ]
    }
   ],
   "source": [
    "set_all_seeds(seed=42)\n",
    "device, n_gpu = initialize_device_settings(use_cuda=False)\n",
    "n_epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "evaluate_every = 500\n",
    "lang_model = \"bert-base-uncased\"\n",
    "do_lower_case = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/01/2020 22:58:34 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
      "07/01/2020 22:58:34 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/rohit/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.load(\n",
    "    pretrained_model_name_or_path=lang_model,\n",
    "    do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/01/2020 22:58:35 - INFO - farm.data_handler.data_silo -   \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "07/01/2020 22:58:35 - INFO - farm.data_handler.data_silo -   Loading train set from: data/toxic-comments/train.tsv \n",
      "07/01/2020 22:58:36 - INFO - farm.data_handler.data_silo -   Got ya 7 parallel workers to convert 159571 dictionaries to pytorch datasets (chunksize = 2000)...\n",
      "07/01/2020 22:58:36 - INFO - farm.data_handler.data_silo -    0    0    0    0    0    0    0 \n",
      "07/01/2020 22:58:36 - INFO - farm.data_handler.data_silo -   /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\\n",
      "07/01/2020 22:58:36 - INFO - farm.data_handler.data_silo -   /'\\  / \\  /'\\  /'\\  / \\  / \\  /'\\\n",
      "07/01/2020 22:58:36 - INFO - farm.data_handler.data_silo -               \n",
      "Preprocessing Dataset data/toxic-comments/train.tsv:   0%|          | 0/159571 [00:00<?, ? Dicts/s]07/01/2020 22:58:52 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "07/01/2020 22:58:52 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 521-0\n",
      "Clear Text: \n",
      " \ttext: Dictionaries\n",
      "\n",
      "How dare you call my contribution spam!!! I am a Kurd and I made a lsit of kurdish dictionaries. you bloody turkish nationalist and atoricity commiting bone breaking Nazi. watch out folk this slimy Turk is trying to censor the internet this is not undemocratic Turkey here, no prison cells in wikipedia you stupid Turk! And you buggers want membership to the EEC\n",
      " \ttext_classification_label: toxic,obscene,insult,identity_hate\n",
      "Tokenized: \n",
      " \ttokens: ['di', '##ction', '##aries', 'how', 'dare', 'you', 'call', 'my', 'contribution', 'spa', '##m', '!', '!', '!', 'i', 'am', 'a', 'ku', '##rd', 'and', 'i', 'made', 'a', 'l', '##sit', 'of', 'kurdish', 'di', '##ction', '##aries', '.', 'you', 'bloody', 'turkish', 'nationalist', 'and', 'at', '##oric', '##ity', 'commit', '##ing', 'bone', 'breaking', 'nazi', '.', 'watch', 'out', 'folk', 'this', 'slim', '##y', 'turk', 'is', 'trying', 'to', 'ce', '##nsor', 'the', 'internet', 'this', 'is', 'not', 'und', '##em', '##oc', '##ratic', 'turkey', 'here', ',', 'no', 'prison', 'cells', 'in', 'wikipedia', 'you', 'stupid', 'turk', '!', 'and', 'you', 'bug', '##gers', 'want', 'membership', 'to', 'the', 'ee', '##c']\n",
      " \toffsets: [0, 2, 7, 14, 18, 23, 27, 32, 35, 48, 51, 52, 53, 54, 56, 58, 61, 63, 65, 68, 72, 74, 79, 81, 82, 86, 89, 97, 99, 104, 109, 111, 115, 122, 130, 142, 146, 148, 152, 156, 162, 166, 171, 180, 184, 186, 192, 196, 201, 206, 210, 212, 217, 220, 227, 230, 232, 237, 241, 250, 255, 258, 262, 265, 267, 269, 275, 282, 286, 288, 291, 298, 304, 307, 317, 321, 328, 332, 334, 338, 342, 345, 350, 355, 366, 369, 373, 375]\n",
      " \tstart_of_word: [True, False, False, True, True, True, True, True, True, True, False, False, False, False, True, True, True, True, False, True, True, True, True, True, False, True, True, True, False, False, False, True, True, True, True, True, True, False, False, True, False, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, False, False, False, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, False]\n",
      "Features: \n",
      " \tinput_ids: [101, 4487, 7542, 12086, 2129, 8108, 2017, 2655, 2026, 6691, 12403, 2213, 999, 999, 999, 1045, 2572, 1037, 13970, 4103, 1998, 1045, 2081, 1037, 1048, 28032, 1997, 15553, 4487, 7542, 12086, 1012, 2017, 6703, 5037, 8986, 1998, 2012, 29180, 3012, 10797, 2075, 5923, 4911, 6394, 1012, 3422, 2041, 5154, 2023, 11754, 2100, 22883, 2003, 2667, 2000, 8292, 29577, 1996, 4274, 2023, 2003, 2025, 6151, 6633, 10085, 23671, 4977, 2182, 1010, 2053, 3827, 4442, 1999, 16948, 2017, 5236, 22883, 999, 1998, 2017, 11829, 15776, 2215, 5779, 2000, 1996, 25212, 2278, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [1, 0, 1, 0, 1, 1]\n",
      "_____________________________________________________\n",
      "07/01/2020 22:58:52 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 591-0\n",
      "Clear Text: \n",
      " \ttext: I never edited the content of your comments, please correct your false accusation.\n",
      " \ttext_classification_label: \n",
      "Tokenized: \n",
      " \ttokens: ['i', 'never', 'edited', 'the', 'content', 'of', 'your', 'comments', ',', 'please', 'correct', 'your', 'false', 'accusation', '.']\n",
      " \toffsets: [0, 2, 8, 15, 19, 27, 30, 35, 43, 45, 52, 60, 65, 71, 81]\n",
      " \tstart_of_word: [True, True, True, True, True, True, True, True, False, True, True, True, True, True, False]\n",
      "Features: \n",
      " \tinput_ids: [101, 1045, 2196, 5493, 1996, 4180, 1997, 2115, 7928, 1010, 3531, 6149, 2115, 6270, 19238, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset data/toxic-comments/train.tsv: 100%|██████████| 159571/159571 [03:20<00:00, 794.64 Dicts/s] \n",
      "07/01/2020 23:01:57 - INFO - farm.data_handler.data_silo -   Loading dev set from: data/toxic-comments/val.tsv\n",
      "07/01/2020 23:01:57 - INFO - farm.data_handler.data_silo -   Got ya 7 parallel workers to convert 10000 dictionaries to pytorch datasets (chunksize = 286)...\n",
      "07/01/2020 23:01:57 - INFO - farm.data_handler.data_silo -    0    0    0    0    0    0    0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/01/2020 23:01:57 - INFO - farm.data_handler.data_silo -   /|\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\\n",
      "07/01/2020 23:01:57 - INFO - farm.data_handler.data_silo -   /'\\  /'\\  /'\\  /'\\  /'\\  / \\  /'\\\n",
      "07/01/2020 23:01:57 - INFO - farm.data_handler.data_silo -               \n",
      "Preprocessing Dataset data/toxic-comments/val.tsv:   0%|          | 0/10000 [00:00<?, ? Dicts/s]07/01/2020 23:02:00 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "07/01/2020 23:02:00 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 230-0\n",
      "Clear Text: \n",
      " \ttext: == Attempts to make it a disambig page == \n",
      "\n",
      " The very intro to the article shows that it cannot be a valid disambig page, per WP:MOSDAB. Because whatever title for the disambig page you select, half of entries will not fit it.\n",
      " \ttext_classification_label: \n",
      "Tokenized: \n",
      " \ttokens: ['=', '=', 'attempts', 'to', 'make', 'it', 'a', 'di', '##sam', '##bi', '##g', 'page', '=', '=', 'the', 'very', 'intro', 'to', 'the', 'article', 'shows', 'that', 'it', 'cannot', 'be', 'a', 'valid', 'di', '##sam', '##bi', '##g', 'page', ',', 'per', 'w', '##p', ':', 'mo', '##sd', '##ab', '.', 'because', 'whatever', 'title', 'for', 'the', 'di', '##sam', '##bi', '##g', 'page', 'you', 'select', ',', 'half', 'of', 'entries', 'will', 'not', 'fit', 'it', '.']\n",
      " \toffsets: [0, 1, 3, 12, 15, 20, 23, 25, 27, 30, 32, 34, 39, 40, 45, 49, 54, 60, 63, 67, 75, 81, 86, 89, 96, 99, 101, 107, 109, 112, 114, 116, 120, 122, 126, 127, 128, 129, 131, 133, 135, 137, 145, 154, 160, 164, 168, 170, 173, 175, 177, 182, 186, 192, 194, 199, 202, 210, 215, 219, 223, 225]\n",
      " \tstart_of_word: [True, False, True, True, True, True, True, True, False, False, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, False, True, True, False, False, False, False, False, False, True, True, True, True, True, True, False, False, False, True, True, True, False, True, True, True, True, True, True, True, False]\n",
      "Features: \n",
      " \tinput_ids: [101, 1027, 1027, 4740, 2000, 2191, 2009, 1037, 4487, 21559, 5638, 2290, 3931, 1027, 1027, 1996, 2200, 17174, 2000, 1996, 3720, 3065, 2008, 2009, 3685, 2022, 1037, 9398, 4487, 21559, 5638, 2290, 3931, 1010, 2566, 1059, 2361, 1024, 9587, 16150, 7875, 1012, 2138, 3649, 2516, 2005, 1996, 4487, 21559, 5638, 2290, 3931, 2017, 7276, 1010, 2431, 1997, 10445, 2097, 2025, 4906, 2009, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "07/01/2020 23:02:00 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 110-0\n",
      "Clear Text: \n",
      " \ttext: Never heard of it.  Is this a joke like the idea of someone putting a knife in a box of Wheaties so they can be a cereal killer?\n",
      " \ttext_classification_label: \n",
      "Tokenized: \n",
      " \ttokens: ['never', 'heard', 'of', 'it', '.', 'is', 'this', 'a', 'joke', 'like', 'the', 'idea', 'of', 'someone', 'putting', 'a', 'knife', 'in', 'a', 'box', 'of', 'wheat', '##ies', 'so', 'they', 'can', 'be', 'a', 'cereal', 'killer', '?']\n",
      " \toffsets: [0, 6, 12, 15, 17, 20, 23, 28, 30, 35, 40, 44, 49, 52, 60, 68, 70, 76, 79, 81, 85, 88, 93, 97, 100, 105, 109, 112, 114, 121, 127]\n",
      " \tstart_of_word: [True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False]\n",
      "Features: \n",
      " \tinput_ids: [101, 2196, 2657, 1997, 2009, 1012, 2003, 2023, 1037, 8257, 2066, 1996, 2801, 1997, 2619, 5128, 1037, 5442, 1999, 1037, 3482, 1997, 10500, 3111, 2061, 2027, 2064, 2022, 1037, 20943, 6359, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset data/toxic-comments/val.tsv: 100%|██████████| 10000/10000 [00:14<00:00, 673.57 Dicts/s]\n",
      "07/01/2020 23:02:12 - INFO - farm.data_handler.data_silo -   No test set is being loaded\n",
      "07/01/2020 23:02:12 - INFO - farm.data_handler.data_silo -   Examples in train: 159571\n",
      "07/01/2020 23:02:12 - INFO - farm.data_handler.data_silo -   Examples in dev  : 10000\n",
      "07/01/2020 23:02:12 - INFO - farm.data_handler.data_silo -   Examples in test : 0\n",
      "07/01/2020 23:02:12 - INFO - farm.data_handler.data_silo -   \n",
      "07/01/2020 23:02:12 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     128\n",
      "07/01/2020 23:02:12 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 63.34102061151462\n",
      "07/01/2020 23:02:12 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.1950417055730678\n"
     ]
    }
   ],
   "source": [
    "label_list = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "metric = \"acc\"\n",
    "\n",
    "processor = TextClassificationProcessor(tokenizer=tokenizer,\n",
    "                                        max_seq_len=128,\n",
    "                                        data_dir=Path(\"./data/toxic-comments\"),\n",
    "                                        label_list=label_list,\n",
    "                                        label_column_name=\"label\",\n",
    "                                        metric=metric,\n",
    "                                        quote_char='\"',\n",
    "                                        multilabel=True,\n",
    "                                        train_filename=\"train.tsv\",\n",
    "                                        dev_filename=\"val.tsv\",\n",
    "                                        test_filename=None,\n",
    "                                        dev_split=0,\n",
    "                                        )\n",
    "\n",
    "# 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n",
    "data_silo = DataSilo(\n",
    "    processor=processor,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model and Prediction Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/01/2020 23:03:33 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/rohit/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "07/01/2020 23:03:36 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "07/01/2020 23:03:36 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 6]\n",
      "07/01/2020 23:03:36 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 3e-05}'\n",
      "07/01/2020 23:03:36 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
      "07/01/2020 23:03:36 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 498.70000000000005, 'num_training_steps': 4987}'\n"
     ]
    }
   ],
   "source": [
    "# 4. Create an AdaptiveModel\n",
    "# a) which consists of a pretrained language model as a basis\n",
    "language_model = LanguageModel.load(lang_model)\n",
    "# b) and a prediction head on top that is suited for our task => Text classification\n",
    "prediction_head = MultiLabelTextClassificationHead(num_labels=len(label_list))\n",
    "\n",
    "model = AdaptiveModel(\n",
    "    language_model=language_model,\n",
    "    prediction_heads=[prediction_head],\n",
    "    embeds_dropout_prob=0.1,\n",
    "    lm_output_types=[\"per_sequence\"],\n",
    "    device=device)\n",
    "\n",
    "# 5. Create an optimizer\n",
    "model, optimizer, lr_schedule = initialize_optimizer(\n",
    "    model=model,\n",
    "    learning_rate=3e-5,\n",
    "    device=device,\n",
    "    n_batches=len(data_silo.loaders[\"train\"]),\n",
    "    n_epochs=n_epochs)\n",
    "\n",
    "# 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    data_silo=data_silo,\n",
    "    epochs=n_epochs,\n",
    "    n_gpu=n_gpu,\n",
    "    lr_schedule=lr_schedule,\n",
    "    evaluate_every=evaluate_every,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/01/2020 23:07:51 - INFO - farm.train -   \n",
      " \n",
      "\n",
      "          &&& &&  & &&             _____                   _             \n",
      "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
      "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
      "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
      "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
      "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
      " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
      "     &&     \\|||                                                   |___/\n",
      "             |||\n",
      "             |||\n",
      "             |||\n",
      "       , -=-~  .-^- _\n",
      "              `\n",
      "\n",
      "Train epoch 0/0 (Cur. train loss: 0.8174):   0%|          | 1/4987 [00:10<14:13:40, 10.27s/it]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Hooray! You have a model. Store it:\n",
    "save_dir = Path(\"../models/bert-multi-toxic-comment\")\n",
    "model.save(save_dir)\n",
    "processor.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
