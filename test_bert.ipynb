{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farm.utils import initialize_device_settings\n",
    "from farm.modeling.tokenization import Tokenizer\n",
    "from farm.data_handler.processor import TextClassificationProcessor, SquadProcessor\n",
    "from farm.data_handler.data_silo import DataSilo\n",
    "from farm.eval import Evaluator\n",
    "from farm.modeling.adaptive_model import AdaptiveModel\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/07/2020 01:20:06 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n"
     ]
    }
   ],
   "source": [
    "device, n_gpu = initialize_device_settings(use_cuda=False)\n",
    "lang_model = \"./models/bert-multi-toxic-comment\"\n",
    "do_lower_case = True\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../data/toxic-comments\")\n",
    "evaluation_filename = \"val.tsv\"\n",
    "label_list = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "metric = \"f1_macro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/07/2020 01:20:06 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
      "07/07/2020 01:20:06 - INFO - transformers.tokenization_utils -   Model name './models/bert-multi-toxic-comment' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming './models/bert-multi-toxic-comment' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/07/2020 01:20:06 - INFO - transformers.tokenization_utils -   Didn't find file ./models/bert-multi-toxic-comment/added_tokens.json. We won't load it.\n",
      "07/07/2020 01:20:06 - INFO - transformers.tokenization_utils -   loading file ./models/bert-multi-toxic-comment/vocab.txt\n",
      "07/07/2020 01:20:06 - INFO - transformers.tokenization_utils -   loading file None\n",
      "07/07/2020 01:20:06 - INFO - transformers.tokenization_utils -   loading file ./models/bert-multi-toxic-comment/special_tokens_map.json\n",
      "07/07/2020 01:20:06 - INFO - transformers.tokenization_utils -   loading file ./models/bert-multi-toxic-comment/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# 1.Create a tokenizer\n",
    "tokenizer = Tokenizer.load(\n",
    "    pretrained_model_name_or_path=lang_model,\n",
    "    do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TextClassificationProcessor(tokenizer=tokenizer,\n",
    "                                        max_seq_len=128,\n",
    "                                        data_dir=Path(\"./data/toxic-comments\"),\n",
    "                                        label_list=label_list,\n",
    "                                        label_column_name=\"label\",\n",
    "                                        metric=metric,\n",
    "                                        quote_char='\"',\n",
    "                                        multilabel=True,\n",
    "                                        train_filename=None,\n",
    "                                        dev_filename=None,\n",
    "                                        dev_split=0,\n",
    "                                        test_filename=evaluation_filename\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/07/2020 01:20:07 - INFO - farm.data_handler.data_silo -   \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "07/07/2020 01:20:07 - INFO - farm.data_handler.data_silo -   No train set is being loaded\n",
      "07/07/2020 01:20:07 - INFO - farm.data_handler.data_silo -   No dev set is being loaded\n",
      "07/07/2020 01:20:07 - INFO - farm.data_handler.data_silo -   Loading test set from: data/toxic-comments/val.tsv\n",
      "07/07/2020 01:20:07 - INFO - farm.data_handler.data_silo -   Got ya 7 parallel workers to convert 10000 dictionaries to pytorch datasets (chunksize = 286)...\n",
      "07/07/2020 01:20:07 - INFO - farm.data_handler.data_silo -    0    0    0    0    0    0    0 \n",
      "07/07/2020 01:20:07 - INFO - farm.data_handler.data_silo -   /w\\  /w\\  /w\\  /|\\  /w\\  /w\\  /|\\\n",
      "07/07/2020 01:20:07 - INFO - farm.data_handler.data_silo -   /'\\  / \\  /'\\  /'\\  /'\\  /'\\  /'\\\n",
      "07/07/2020 01:20:07 - INFO - farm.data_handler.data_silo -               \n",
      "Preprocessing Dataset data/toxic-comments/val.tsv:   0%|          | 0/10000 [00:00<?, ? Dicts/s]07/07/2020 01:20:09 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "07/07/2020 01:20:09 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 233-0\n",
      "Clear Text: \n",
      " \ttext: *They cheated by using 10 year olds.\n",
      " \ttext_classification_label: \n",
      "Tokenized: \n",
      " \ttokens: ['*', 'they', 'cheated', 'by', 'using', '10', 'year', 'olds', '.']\n",
      " \toffsets: [0, 1, 6, 14, 17, 23, 26, 31, 35]\n",
      " \tstart_of_word: [True, False, True, True, True, True, True, True, False]\n",
      "Features: \n",
      " \tinput_ids: [101, 1008, 2027, 22673, 2011, 2478, 2184, 2095, 19457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "07/07/2020 01:20:09 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 71-0\n",
      "Clear Text: \n",
      " \ttext: ==Requested move== \n",
      "\n",
      "  \n",
      " :Bobby Harrell, Jr. →  – Simplify, his father isn't famous.    \n",
      "\n",
      " ===Survey=== \n",
      " :Feel free to state your position on the renaming proposal by beginning a new line in this section with *'''Support''' or *'''Oppose''', then sign your comment with ~~~~. Since polling is not a substitute for discussion, please explain your reasons, taking into account Wikipedia's policy on article titles. \n",
      "\n",
      " * \n",
      "\n",
      " ===Discussion=== \n",
      " :Any additional comments:\n",
      " \ttext_classification_label: \n",
      "Tokenized: \n",
      " \ttokens: ['=', '=', 'requested', 'move', '=', '=', ':', 'bobby', 'ha', '##rrell', ',', 'jr', '.', '→', '–', 'sim', '##plify', ',', 'his', 'father', 'isn', \"'\", 't', 'famous', '.', '=', '=', '=', 'survey', '=', '=', '=', ':', 'feel', 'free', 'to', 'state', 'your', 'position', 'on', 'the', 'renaming', 'proposal', 'by', 'beginning', 'a', 'new', 'line', 'in', 'this', 'section', 'with', '*', \"'\", \"'\", \"'\", 'support', \"'\", \"'\", \"'\", 'or', '*', \"'\", \"'\", \"'\", 'oppose', \"'\", \"'\", \"'\", ',', 'then', 'sign', 'your', 'comment', 'with', '~', '~', '~', '~', '.', 'since', 'polling', 'is', 'not', 'a', 'substitute', 'for', 'discussion', ',', 'please', 'explain', 'your', 'reasons', ',', 'taking', 'into', 'account', 'wikipedia', \"'\", 's', 'policy', 'on', 'article', 'titles', '.', '*', '=', '=', '=', 'discussion', '=', '=', '=', ':', 'any', 'additional', 'comments', ':']\n",
      " \toffsets: [0, 1, 2, 12, 16, 17, 25, 26, 32, 34, 39, 41, 43, 45, 48, 50, 53, 58, 60, 64, 71, 74, 75, 77, 83, 91, 92, 93, 94, 100, 101, 102, 106, 107, 112, 117, 120, 126, 131, 140, 143, 147, 156, 165, 168, 178, 180, 184, 189, 192, 197, 205, 210, 211, 212, 213, 214, 221, 222, 223, 225, 228, 229, 230, 231, 232, 238, 239, 240, 241, 243, 248, 253, 258, 266, 271, 272, 273, 274, 275, 277, 283, 291, 294, 298, 300, 311, 315, 325, 327, 334, 342, 347, 354, 356, 363, 368, 376, 385, 386, 388, 395, 398, 406, 412, 417, 422, 423, 424, 425, 435, 436, 437, 441, 442, 446, 457, 465]\n",
      " \tstart_of_word: [True, False, False, True, False, False, True, False, True, False, False, True, False, True, True, True, False, False, True, True, True, False, False, True, False, True, False, False, False, False, False, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, True, True, True, True, False, False, False, False, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, False, False, True, True, True, True, False, True, True, False, False, False, False, False, False, True, False, True, True, False]\n",
      "Features: \n",
      " \tinput_ids: [101, 1027, 1027, 7303, 2693, 1027, 1027, 1024, 6173, 5292, 14069, 1010, 3781, 1012, 1585, 1516, 21934, 28250, 1010, 2010, 2269, 3475, 1005, 1056, 3297, 1012, 1027, 1027, 1027, 5002, 1027, 1027, 1027, 1024, 2514, 2489, 2000, 2110, 2115, 2597, 2006, 1996, 24944, 6378, 2011, 2927, 1037, 2047, 2240, 1999, 2023, 2930, 2007, 1008, 1005, 1005, 1005, 2490, 1005, 1005, 1005, 2030, 1008, 1005, 1005, 1005, 15391, 1005, 1005, 1005, 1010, 2059, 3696, 2115, 7615, 2007, 1066, 1066, 1066, 1066, 1012, 2144, 17888, 2003, 2025, 1037, 7681, 2005, 6594, 1010, 3531, 4863, 2115, 4436, 1010, 2635, 2046, 4070, 16948, 1005, 1055, 3343, 2006, 3720, 4486, 1012, 1008, 1027, 1027, 1027, 6594, 1027, 1027, 1027, 1024, 2151, 3176, 7928, 1024, 102, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Dataset data/toxic-comments/val.tsv: 100%|██████████| 10000/10000 [00:09<00:00, 1045.53 Dicts/s]\n",
      "07/07/2020 01:20:17 - INFO - farm.data_handler.data_silo -   Examples in train: 0\n",
      "07/07/2020 01:20:17 - INFO - farm.data_handler.data_silo -   Examples in dev  : 0\n",
      "07/07/2020 01:20:17 - INFO - farm.data_handler.data_silo -   Examples in test : 10000\n",
      "07/07/2020 01:20:17 - INFO - farm.data_handler.data_silo -   \n"
     ]
    }
   ],
   "source": [
    "# 3. Create a DataSilo that loads dataset, provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n",
    "data_silo = DataSilo(\n",
    "    processor=processor,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create an Evaluator\n",
    "evaluator = Evaluator(\n",
    "    data_loader=data_silo.get_data_loader(\"test\"),\n",
    "    tasks=data_silo.processor.tasks,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/07/2020 01:20:17 - INFO - transformers.modeling_utils -   loading weights file models/bert-multi-toxic-comment/language_model.bin from cache at models/bert-multi-toxic-comment/language_model.bin\n",
      "07/07/2020 01:20:19 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads\n",
      "07/07/2020 01:20:19 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
      "07/07/2020 01:20:19 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 6]\n",
      "07/07/2020 01:20:19 - INFO - farm.modeling.prediction_head -   Loading prediction head from models/bert-multi-toxic-comment/prediction_head_0.bin\n",
      "Evaluating: 100%|██████████| 313/313 [17:35<00:00,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaged F1-Score: 0.6080237118447319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/rohit/anaconda3/envs/mlnlp/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/mlnlp/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# 5. Load model\n",
    "# model = AdaptiveModel.convert_from_transformers(lang_model, device=device, task_type=\"text_classification\")\n",
    "# use \"load\" if you want to use a local model that was trained with FARM\n",
    "model = AdaptiveModel.load(lang_model, device=device)\n",
    "model.connect_heads_with_processor(data_silo.processor.tasks, require_labels=True)\n",
    "\n",
    "# 6. Run the Evaluator\n",
    "results = evaluator.eval(model)\n",
    "f1_score = results[0][\"f1_macro\"]\n",
    "print(\"Macro-averaged F1-Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlnlp]",
   "language": "python",
   "name": "conda-env-mlnlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
